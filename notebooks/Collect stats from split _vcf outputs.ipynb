{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mafs\n",
    "/vol/sci/bio/data/gil.greenbaum/amir.rubin/vcf/hgdp/classes/chr1/maf_0.01.012  /vol/sci/bio/data/gil.greenbaum/amir.rubin/vcf/hgdp/classes/chr1/maf_0.01.012.indv  /vol/sci/bio/data/gil.greenbaum/amir.rubin/vcf/hgdp/classes/chr1/maf_0.01.012.pos\n",
    "\n",
    "macs\n",
    "/vol/sci/bio/data/gil.greenbaum/amir.rubin/vcf/hgdp/classes/chr1/mac_2.012  /vol/sci/bio/data/gil.greenbaum/amir.rubin/vcf/hgdp/classes/chr1/mac_2.012.indv  /vol/sci/bio/data/gil.greenbaum/amir.rubin/vcf/hgdp/classes/chr1/mac_2.012.pos\n",
    "\n",
    "012 file: (big file)\n",
    "\t929 lines\n",
    "\tcolumns - check with stderr!\n",
    "\t<indv index><tab>0/1/2<tab>0/1/2....\n",
    "\n",
    "indv file:\n",
    "\t929 lines\n",
    "\tnames of indv:\n",
    "\tHGDP01201\n",
    "\tHGDP00479\n",
    "\tHGDP01408\n",
    "\tHGDP00471\n",
    "\t...\n",
    "\t\n",
    "pos file:\n",
    "\tnumber of lines- same as number of columns in 012 (+-1) \n",
    "\tchr1    14414\n",
    "\tchr1    15515\n",
    "\tchr1    15750\n",
    "\tchr1    16690\n",
    "\tchr1    16711\n",
    "\t...\n",
    "\t\n",
    "\t\n",
    "stdout file - no details.\n",
    "\t\n",
    "stderr file:\n",
    "\t/vol/sci/bio/data/gil.greenbaum/amir.rubin/logs/cluster/split_vcfs/stderr/chr19_mac2.stderr\n",
    "\textract:\n",
    "\tinput_file\n",
    "\tmac\n",
    "\tmax_mac\n",
    "\tmaf\n",
    "\tmax_maf\n",
    "\tout_folder\n",
    "\tnum_of_indv_after_filter\n",
    "\tnum_of_sites_after_filter\n",
    "\tnum_of_possible_sites\n",
    "\trun_time\n",
    "\n",
    "\t\n",
    "VCFtools - 0.1.16\n",
    "(C) Adam Auton and Anthony Marcketta 2009\n",
    "\n",
    "Parameters as interpreted:\n",
    "        --gzvcf /vol/sci/bio/data/gil.greenbaum/amir.rubin/vcf/hgdp/hgdp_wgs.20190516.full.chr19.vcf.gz\n",
    "        --mac 3\n",
    "        --max-alleles 2\n",
    "        --max-mac 3\n",
    "        --min-alleles 2\n",
    "        --max-missing 0.9\n",
    "        --012\n",
    "        --out /vol/sci/bio/data/gil.greenbaum/amir.rubin/vcf/hgdp/classes/chr19/mac_3\n",
    "        --remove-indels\n",
    "        --temp /vol/sci/bio/data/gil.greenbaum/amir.rubin/vcf/hgdp/classes/chr19/temp_mac_3\n",
    "\n",
    "Using zlib version: 1.2.11\n",
    "Warning: Expected at least 2 parts in FORMAT entry: ID=PGT,Number=1,Type=String,Description=\"Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another\">\n",
    "Warning: Expected at least 2 parts in FORMAT entry: ID=PID,Number=1,Type=String,Description=\"Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group\">\n",
    "Warning: Expected at least 2 parts in FORMAT entry: ID=PL,Number=G,Type=Integer,Description=\"Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification\">\n",
    "Warning: Expected at least 2 parts in FORMAT entry: ID=RGQ,Number=1,Type=Integer,Description=\"Unconditional reference genotype confidence, encoded as a phred quality -10*log10 p(genotype call is wrong)\">\n",
    "Warning: Expected at least 2 parts in INFO entry: ID=AC,Number=A,Type=Integer,Description=\"Allele count in genotypes, for each ALT allele, in the same order as listed\">\n",
    "Warning: Expected at least 2 parts in INFO entry: ID=AC,Number=A,Type=Integer,Description=\"Allele count in genotypes, for each ALT allele, in the same order as listed\">\n",
    "Warning: Expected at least 2 parts in INFO entry: ID=AF,Number=A,Type=Float,Description=\"Allele Frequency, for each ALT allele, in the same order as listed\">\n",
    "Warning: Expected at least 2 parts in INFO entry: ID=AF,Number=A,Type=Float,Description=\"Allele Frequency, for each ALT allele, in the same order as listed\">\n",
    "Warning: Expected at least 2 parts in INFO entry: ID=MLEAC,Number=A,Type=Integer,Description=\"Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed\">\n",
    "Warning: Expected at least 2 parts in INFO entry: ID=MLEAC,Number=A,Type=Integer,Description=\"Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed\">\n",
    "Warning: Expected at least 2 parts in INFO entry: ID=MLEAF,Number=A,Type=Float,Description=\"Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed\">\n",
    "Warning: Expected at least 2 parts in INFO entry: ID=MLEAF,Number=A,Type=Float,Description=\"Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed\">\n",
    "Warning: Expected at least 2 parts in INFO entry: ID=culprit,Number=1,Type=String,Description=\"The annotation which was the worst performing in the Gaussian mixture model, likely the reason why the variant was filtered out\">\n",
    "Warning: Expected at least 2 parts in INFO entry: ID=VQSRMODE,Number=1,Type=String,Description=\"The mode in which VQSR was run, INDEL or SNP\">\n",
    "Warning: Expected at least 2 parts in INFO entry: ID=AA_chimp,Number=1,Type=String,Description=\"Ancestral Allele, as extracted from the GRCh38-PanTro4 alignment from UCSC\">\n",
    "Warning: Expected at least 2 parts in INFO entry: ID=AA_ensembl,Number=1,Type=String,Description=\"Ancestral Allele, as extracted from Ensembl's homo_sapiens_ancestor_GRCh38_e86 files, database: cc21_ensembl_compara_86@compara5:3306, MethodLinkSpeciesSet: 8 primates EPO (822)\">\n",
    "Warning: Expected at least 2 parts in INFO entry: ID=AA_ensembl,Number=1,Type=String,Description=\"Ancestral Allele, as extracted from Ensembl's homo_sapiens_ancestor_GRCh38_e86 files, database: cc21_ensembl_compara_86@compara5:3306, MethodLinkSpeciesSet: 8 primates EPO (822)\">\n",
    "Warning: Expected at least 2 parts in INFO entry: ID=AA_ensembl,Number=1,Type=String,Description=\"Ancestral Allele, as extracted from Ensembl's homo_sapiens_ancestor_GRCh38_e86 files, database: cc21_ensembl_compara_86@compara5:3306, MethodLinkSpeciesSet: 8 primates EPO (822)\">\n",
    "After filtering, kept 929 out of 929 Individuals\n",
    "Writing 012 matrix files ... Done.\n",
    "After filtering, kept 85588 out of a possible 1759281 Sites\n",
    "Run Time = 970.00 seconds\n",
    "seq: invalid floating point argument: ‘-’\n",
    "Try 'seq --help' for more information.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T15:33:56.047282Z",
     "start_time": "2021-01-13T15:33:56.004281Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import os.path\n",
    "\n",
    "def get_split_vcf_stats(logs_folder, log_file, chr_name_name):\n",
    "    # extract stats from the stderr file\n",
    "    # return a dictionary\n",
    "    filepath = logs_folder + log_file\n",
    "    values = dict()\n",
    "    values['chr_name_name'] = chr_name_name\n",
    "    values['mac'] = '-'\n",
    "    values['maf'] = '-'\n",
    "    values['max_mac'] = '-'\n",
    "    values['max_maf'] = '-'\n",
    "    #regexes\n",
    "\n",
    "    # After filtering, kept 929 out of 929 Individuals\n",
    "    r = r'After filtering, kept (\\d+) out of (\\d+) Individuals'\n",
    "    indv_regex = re.compile(r)\n",
    "    # After filtering, kept 3202 out of a possible 1185008 Sites\n",
    "    r = r'After filtering, kept (\\d+) out of a possible (\\d+) Sites'\n",
    "    sites_regex = re.compile(r)\n",
    "    # Run Time = 489.00 seconds\n",
    "    r = r'Run Time = (\\d+).00 seconds'\n",
    "    time_regex = re.compile(r)\n",
    "\n",
    "    r = r'After filtering, kept (\\d+) out of (\\d+) Individuals'\n",
    "    indv_regex = re.compile(r)\n",
    "    with open(filepath) as fp:\n",
    "        for cnt, line in enumerate(fp):\n",
    "            if '--gzvcf ' in line:\n",
    "                values['input_file'] = line.split('--gzvcf ')[1].strip()\n",
    "            elif '--mac ' in line:\n",
    "                values['mac'] = line.split('--mac ')[1].strip()\n",
    "            elif '--max-mac ' in line:\n",
    "                values['max_mac'] = line.split('--max-mac ')[1].strip()\n",
    "            elif '--maf ' in line:\n",
    "                values['maf'] = line.split('--maf ')[1].strip()\n",
    "            elif '--max-maf ' in line:\n",
    "                values['max_maf'] = line.split('--max-maf ')[1].strip()\n",
    "            elif '--out ' in line:\n",
    "                values['out_path'] = line.split('--out ')[1].strip()\n",
    "            else:\n",
    "                match = indv_regex.match(line)\n",
    "                if match:\n",
    "                    values['num_of_indv_after_filter'] = match.groups()[0]\n",
    "                    values['num_of_possible_indv'] = match.groups()[1]\n",
    "                else:\n",
    "                    match = sites_regex.match(line)\n",
    "                    if match:\n",
    "                        values['num_of_sites_after_filter'] = match.groups()[0]\n",
    "                        values['num_of_possible_sites'] = match.groups()[1]\n",
    "                    else:\n",
    "                        match = time_regex.match(line)\n",
    "                        if match:\n",
    "                            values['run_time_in_seconds'] = match.groups()[0]\n",
    "\n",
    "    # based on the values we analyze the output files\n",
    "    # analyze indv file\n",
    "    indv_file = values['out_path'] + '.012.indv'\n",
    "    #TODO remove\n",
    "    indv_file = r\"C:\\Data\\HUJI\\hgdp\\mock\\chr22/\" + values['out_path'].split('/')[-1] +  \".012.indv\"\n",
    "    values['indv_num_of_lines'] = number_of_lines(indv_file)\n",
    "\n",
    "    pos_file = values['out_path'] + '.012.pos'\n",
    "    #TODO remove\n",
    "    pos_file = r\"C:\\Data\\HUJI\\hgdp\\mock\\chr22/\" + values['out_path'].split('/')[-1] +  \".012.pos\"\n",
    "    values['pos_num_of_lines'] = number_of_lines(pos_file)\n",
    "\n",
    "    main_file = values['out_path'] + '.012'\n",
    "    #TODO remove\n",
    "    main_file = r\"C:\\Data\\HUJI\\hgdp\\mock\\chr22/\" + values['out_path'].split('/')[-1] +  \".012\"\n",
    "    values['012_num_of_lines'] = number_of_lines(main_file)\n",
    "    min_c, max_c = min_max_number_of_columns(main_file)\n",
    "    # substract 1 as we have an index column\n",
    "    values['012_min_num_of_sites'] = min_c-1\n",
    "    values['012_max_num_of_sites'] = max_c-1\n",
    "    return values\n",
    "\n",
    "\n",
    "def number_of_lines(file_path):\n",
    "    count = 0\n",
    "    for line in open(file_path).readlines(): count += 1\n",
    "    return count\n",
    "\n",
    "def min_max_number_of_columns(file_path):\n",
    "    min_c = sys.maxsize\n",
    "    max_c = -1\n",
    "    for line in open(file_path).readlines():\n",
    "        c = len(line.split('\\t'))\n",
    "        if c < min_c:\n",
    "            min_c = c\n",
    "        if c > max_c:\n",
    "            max_c = c\n",
    "    return min_c, max_c\n",
    "\n",
    "def write_values_to_csv(values, output_path):\n",
    "    # first, assert we have all values\n",
    "    expected_keys = ['chr_name_name', 'mac', 'max_mac', 'maf', 'max_maf', 'num_of_indv_after_filter', 'indv_num_of_lines', '012_num_of_lines', 'num_of_possible_indv', 'num_of_sites_after_filter', 'pos_num_of_lines', '012_min_num_of_sites', '012_max_num_of_sites', 'num_of_possible_sites', 'run_time_in_seconds', 'input_file', 'out_path']\n",
    "    values_keys = values.keys()\n",
    "    for exp_key in expected_keys:\n",
    "        assert exp_key in values_keys\n",
    "    write_header = not os.path.isfile(output_path)\n",
    "    with open(output_path, \"a+\") as f:\n",
    "        if write_header:\n",
    "            f.write(','.join(expected_keys) + '\\n')\n",
    "        # if file doesnt exist, write the header\n",
    "        f.write(','.join([str(values[k]) for k in expected_keys]) + '\\n')\n",
    "        \n",
    "def collect_split_vcf_stats(logs_folder, log_files, chr_name_names, split_vcf_stats_csv_path):\n",
    "    assert len(log_files) == len(chr_name_names)\n",
    "    for i in range(len(log_files)):\n",
    "        chr_name_name = chr_name_names[i]\n",
    "        log_file = log_files[i]\n",
    "        values = get_split_vcf_stats(logs_folder, log_file, chr_name_name)\n",
    "        write_values_to_csv(values, split_vcf_stats_csv_path)\n",
    "        print(f'done with file {i} out of {len(log_files)}')\n",
    "\n",
    "def call_collect_split_vcf_stats(logs_folder, chr_names, split_vcf_stats_csv_path, min_mac_range, max_mac_range, mac_delta, min_maf_range, max_maf_range, maf_delta):\n",
    "    macs = range(min_mac_range,max_mac_range+1, mac_delta)\n",
    "    mafs = [float(v)/100 for v in range(min_maf_range,max_maf_range+1, maf_delta)]\n",
    "    log_files = []\n",
    "    chr_names_for_logs = []\n",
    "    for chr_name in chr_names:\n",
    "        for mac in macs:\n",
    "            log_files.append(f'{chr_name}_mac{mac}.stderr')\n",
    "            chr_names_for_logs.append(chr_name)\n",
    "        for maf in mafs:\n",
    "            log_files.append(f'{chr_name}_maf{maf}.stderr')\n",
    "            chr_names_for_logs.append(chr_name)\n",
    "    print(f'will process {len(log_files)} files')\n",
    "    collect_split_vcf_stats(logs_folder, log_files, chr_names_for_logs, split_vcf_stats_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T15:35:06.668940Z",
     "start_time": "2021-01-13T15:35:06.402868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will process 2 files\n",
      "done with file 0 out of 2\n",
      "done with file 1 out of 2\n"
     ]
    }
   ],
   "source": [
    "logs_folder = r'C:\\Data\\HUJI\\hgdp\\mock\\chr22\\stderr/'\n",
    "chr_names = [f'chr{i}' for i in range(22,23)]\n",
    "split_vcf_stats_csv_path = r'C:\\Data\\HUJI\\hgdp\\mock\\chr22\\vcf_stats.csv'\n",
    "\n",
    "call_collect_split_vcf_stats(logs_folder, chr_names, split_vcf_stats_csv_path, min_mac_range=18, max_mac_range=18, mac_delta=1, min_maf_range=48, max_maf_range=48, maf_delta=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T15:45:21.326402Z",
     "start_time": "2021-01-13T15:45:21.311404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.9\n"
     ]
    }
   ],
   "source": [
    "myFloat = 23.9;\n",
    "print(myFloat, 2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T15:46:11.496700Z",
     "start_time": "2021-01-13T15:46:11.474695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23.90'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{0:.2f}\".format(myFloat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
