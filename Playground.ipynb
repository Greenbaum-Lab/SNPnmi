{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T13:40:34.248113Z",
     "start_time": "2020-12-16T13:40:34.203130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Data/HUJI/hgdp/'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.config import *\n",
    "local_data_folder = get_local_data_folder('hgdp')\n",
    "local_data_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T13:40:37.970067Z",
     "start_time": "2020-12-16T13:40:37.963031Z"
    }
   },
   "outputs": [],
   "source": [
    "# todo - think about the infra to get to each file, and naming conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process 012 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T13:40:39.835431Z",
     "start_time": "2020-12-16T13:40:38.862920Z"
    }
   },
   "outputs": [],
   "source": [
    "#imports and consts\n",
    "import os.path\n",
    "import logging\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "SUFFIX_POS = '.012.pos'\n",
    "SUFFIX_012 = '.012'\n",
    "OUTPUT_PATTERN_DIST_FILE = 'dist_slice_{slice_index}_{num_of_loci}_loci.tsv'\n",
    "OUTPUT_PATTERN_LOCI_FILE = 'loci_ids_slice_{slice_index}.tsv'\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# TODO?\n",
    "#fhandler = logging.FileHandler(filename='Process012.log', mode='a')\n",
    "#formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "#fhandler.setFormatter(formatter)\n",
    "#logger.addHandler(fhandler)\n",
    "\n",
    "def _log(msg, level=logging.INFO):\n",
    "    print(msg)\n",
    "    logger.log(level, msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T13:40:43.534639Z",
     "start_time": "2020-12-16T13:40:43.520605Z"
    }
   },
   "outputs": [],
   "source": [
    "# func params\n",
    "path_to_012_files = 'C:\\Data\\HUJI\\hgdp\\mock\\out'\n",
    "base_output_path = 'C:\\Data\\HUJI\\hgdp\\mock'\n",
    "# if not in range - will raise a warning\n",
    "min_minor_freq_expected = 0.49\n",
    "max_minor_freq_expected = 0.5\n",
    "# if we have 91 loci, we may have a file with 1 loci in it (can be improved if we want..)\n",
    "# TODO - validate with Gili\n",
    "desired_num_of_loci_per_output = 10\n",
    "# TODO - validate with Gili\n",
    "# if we have less than this which are valied (not -1), locus is not included in calc. This will change the \n",
    "min_valid_sites_precentage = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T13:40:44.698377Z",
     "start_time": "2020-12-16T13:40:44.685357Z"
    }
   },
   "outputs": [],
   "source": [
    "def _validate_file_exist(p):\n",
    "    if not os.path.isfile(p):\n",
    "        _log(f'Cant file file {p}', logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T13:40:45.029247Z",
     "start_time": "2020-12-16T13:40:45.014253Z"
    }
   },
   "outputs": [],
   "source": [
    "def _get_chr_name(path_pos):\n",
    "    with open(path_pos, 'r') as f:\n",
    "        l = f.readline()\n",
    "        char_name = l.split('\\t')[0]\n",
    "        return char_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T13:40:45.420237Z",
     "start_time": "2020-12-16T13:40:45.407272Z"
    }
   },
   "outputs": [],
   "source": [
    "def _prepare_output_folder(base_output_path, chr_name):\n",
    "    output_dir = f'{base_output_path}/{chr_name}/'\n",
    "    _log(f'output_dir is {output_dir}')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    return output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T14:55:20.175432Z",
     "start_time": "2020-12-16T14:55:20.160416Z"
    }
   },
   "outputs": [],
   "source": [
    "def _build_pairwise_db(n, v):\n",
    "    return [[v]*(n-i) for i in range (1,n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T14:57:40.198356Z",
     "start_time": "2020-12-16T14:57:40.167821Z"
    }
   },
   "outputs": [],
   "source": [
    "#pairwise_distances = _calc_pairwise_distances_with_guardrails(slice_df, min_valid_sites_precentage, min_minor_freq_expected, max_minor_freq_expected)\n",
    "# for each column, we calc the pairwise distances and add it to the grand total\n",
    "# for performance, we use 2 lists of lists, one for distances and one for counts\n",
    "# maybe wrap in np to sum?\n",
    "slice_pairwise_dist = _build_pairwise_db(len(slice_df), 0.0)\n",
    "slice_pairwise_counts = _build_pairwise_db(len(slice_df), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T14:57:43.334567Z",
     "start_time": "2020-12-16T14:57:43.320511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non ref allele frequency: 0.5\n",
      "Ref allele frequency: 0.5\n",
      "Check guardrails\n",
      "Precentage of valid sites: 0.4951560818083961\n",
      "Minor allele frequency: 0.5\n",
      "Passed guardrails\n"
     ]
    }
   ],
   "source": [
    "# def _calc_pairwise_distances_with_guardrails_locus\n",
    "site_index = 0\n",
    "genotypes = slice_df.iloc[:,site_index].values\n",
    "# get counts\n",
    "num_individuals = len(genotypes)\n",
    "# count the amount of not -1 in alleles\n",
    "num_valid_genotypes = len(genotypes[genotypes!=-1])\n",
    "non_ref_count = sum(genotypes[genotypes>0])\n",
    "ref_count = 2*num_valid_genotypes-non_ref_count\n",
    "non_ref_freq = float(non_ref_count)/(2*num_valid_genotypes)\n",
    "ref_freq = float(ref_count)/(2*num_valid_genotypes)\n",
    "_log(f'Non ref allele frequency: {non_ref_freq}')\n",
    "_log(f'Ref allele frequency: {ref_freq}')\n",
    "assert abs(ref_freq+non_ref_freq-1)<1e-04\n",
    "_check_guardrails(num_individuals, num_valid_genotypes, ref_count, non_ref_count, min_valid_sites_precentage, min_minor_freq_expected, max_minor_freq_expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T14:57:44.565914Z",
     "start_time": "2020-12-16T14:57:44.553929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 (1-f_a)0 + (1-f_r)4\n",
      "0 1 (1-f_a)0 + (1-f_r)2\n",
      "0 2 (1-f_a)0 + (1-f_r)0\n",
      "1 1 (1-f_a)1 + (1-f_r)1\n",
      "1 2 (1-f_a)2 + (1-f_r)0\n",
      "2 2 (1-f_a)4 + (1-f_r)0\n"
     ]
    }
   ],
   "source": [
    "def test(v1_val, v2_val):\n",
    "    print(v1_val, v2_val, f'(1-f_a){(v1_val*v2_val)} + (1-f_r){(2-v1_val)*(2-v2_val)}')\n",
    "for v1 in range(3):\n",
    "    for v2 in range(v1,3):\n",
    "        test(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T14:57:46.460226Z",
     "start_time": "2020-12-16T14:57:46.455670Z"
    }
   },
   "outputs": [],
   "source": [
    "def _calc_dist(i1_val, i2_val, ref_freq, non_ref_freq):\n",
    "    # from VCFtools manual:\n",
    "    # \"Genotypes are represented as 0, 1 and 2, where the number represent that number of non-reference alleles\"\n",
    "    # So - v1_val and v2_val are the amount of non-ref alleles.\n",
    "    # The distance function is:\n",
    "    # 1/4[(1-f_a)(Iac+Iad) + (1-f_b)(Ibc+Ibd)]\n",
    "    # Now, for each combination of v1_val and v2_val, we can compute the distance.\n",
    "        \n",
    "    # 0,0 - v1_val=0 and v2_val=0:\n",
    "    #    1/4[(1-f_ref)(1+1) + (1-f_ref)(1+1)] = 1/4[(1-f_ref)(4)] = (1-f_ref)\n",
    "    \n",
    "    # 0,1 - v1_val=0 and v2_val=1:\n",
    "    #    1/4[(1-f_ref)(1+0) + (1-f_ref)(1+0)] = 1/4[(1-f_ref)(2)] = 1/2(1-f_ref)\n",
    "    \n",
    "    # 0,2 - v1_val=0 and v2_val=2:\n",
    "    #    1/4[(1-f_ref)(0+0) + (1-f_ref)(0+0)] = 0\n",
    "    \n",
    "    # 1,1 - v1_val=1 and v2_val=2:\n",
    "    #    1/4[(1-f_non_ref)(1+0) + (1-f_ref)(1+0)] = 1/4[(1-f_non_ref)+(1-f_ref)]\n",
    "    \n",
    "    # 1,2 - v1_val=1 and v2_val=2:\n",
    "    #    1/4[(1-f_non_ref)(1+1) + (1-f_ref)(0+0)] = 1/4[(1-f_non_ref)(2)] = 1/2(1-f_non_ref)\n",
    "    \n",
    "    # 2,2 - v1_val=2 and v2_val=2:\n",
    "    #    1/4[(1-f_non_ref)(1+1) + (1-f_non_ref)(1+1)] = 1/4[(1-f_non_ref)(4)] = (1-f_non_ref)\n",
    "    \n",
    "    # Also, note that it is symetric:\n",
    "\n",
    "    # 1,0 - v1_val=1 and v2_val=0:\n",
    "    #    1/4[(1-f_ref)(1+1) + (1-f_non_ref)(0+0)] = 1/4[(1-f_ref)(2)] = 1/2(1-f_ref)\n",
    "    \n",
    "    # 2,1 - v1_val=2 and v2_val=1:\n",
    "    #    1/4[(1-f_non_ref)(1+0) + (1-f_non_ref)(0+1)] = 1/4[(1-f_non_ref)(2)] = 1/2(1-f_non_ref)\n",
    "\n",
    "    # 2,0 - v1_val=2 and v2_val=0:\n",
    "    #    1/4[(1-f_non_ref)(0+0) + (1-f_non_ref)(0+0)] = 0\n",
    "    \n",
    "    # formula to use: \n",
    "    #   (1-f_non_ref)(v1_val*v2_val) + (1-f_ref)((2-v1_val)*(2-v2_val))/4\n",
    "    return (1-non_ref_freq)*(i1_val*i2_val) + (1-ref_freq)*((2-i1_val)*(2-i2_val))/4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T14:59:14.248448Z",
     "start_time": "2020-12-16T14:59:14.237895Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.33333333333333"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sec = 0.5\n",
    "10000*sec/(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T14:59:34.261448Z",
     "start_time": "2020-12-16T14:59:33.643150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site index: 0: done with individual 0\n",
      "Site index: 0: done with individual 100\n",
      "Site index: 0: done with individual 200\n",
      "Site index: 0: done with individual 300\n",
      "Site index: 0: done with individual 400\n",
      "Site index: 0: done with individual 500\n",
      "Site index: 0: done with individual 600\n",
      "Site index: 0: done with individual 700\n",
      "Site index: 0: done with individual 800\n",
      "Site index: 0: done with individual 900\n"
     ]
    }
   ],
   "source": [
    "# now when we have all freq and counts, we can start the pairwise comparison\n",
    "# takes 0.5 seconds for one locus. This means ~4 days if we are not seperating the task (which we will)\n",
    "# Note that if we split this by chromozome, the big ones may have around 1000 in a given freq range, \n",
    "# which can take ~8 minutes. 10k will be around 83 minutes - also cool.\n",
    "\n",
    "for i1 in range(num_individuals-1):\n",
    "    i1_val = allels[i1]\n",
    "    # if this entry is not valid for i1, no need to go over all the others, nothing to add to freq nor counts\n",
    "    if i1%100==0:\n",
    "        _log(f'Site index: {site_index}: done with individual {i1}')\n",
    "    if i1_val == -1:\n",
    "        continue\n",
    "    for i2 in range(i1+1, num_individuals):\n",
    "        i2_val = allels[i2]\n",
    "        if i2_val == -1:\n",
    "            continue\n",
    "        else:            \n",
    "            # this is a valid entry, we add 1 to the count\n",
    "            slice_pairwise_counts[i1][i2-i1-1] += 1\n",
    "            slice_pairwise_dist[i1][i2-i1-1] += _calc_dist(i1_val, i2_val, ref_freq, non_ref_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T14:57:29.405338Z",
     "start_time": "2020-12-16T14:57:29.396340Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 2.0, 2.0],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.0, 0.0],\n",
       " [2.0]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_pairwise_dist[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T13:40:59.281038Z",
     "start_time": "2020-12-16T13:40:59.273078Z"
    }
   },
   "outputs": [],
   "source": [
    "def _check_guardrails(num_individuals, num_valid_genotypes, ref_count, non_ref_count, min_valid_sites_precentage, min_minor_freq_expected, max_minor_freq_expected):\n",
    "    _log(f'Check guardrails')\n",
    "    # guardrail #1 - min_valid_sites_precentage\n",
    "    percentage_valid_sites = float(num_valid_genotypes)/num_individuals\n",
    "    _log(f'Precentage of valid sites: {percentage_valid_sites}')\n",
    "    if percentage_valid_sites < min_valid_sites_precentage:\n",
    "        _log(f'ERROR: % of valid sites is {percentage_valid_sites}, lower than allowd: {min_valid_sites_precentage}.', logging.ERROR)\n",
    "    # guardrail #2 - min_minor_freq_expected and max_minor_freq_expected\n",
    "    minor_count = min(ref_count, non_ref_count)\n",
    "    minor_freq = float(minor_count)/(2*num_valid_genotypes)\n",
    "    _log(f'Minor allele frequency: {minor_freq}')\n",
    "    if minor_freq < min_minor_freq_expected:\n",
    "        _log(f'ERROR: minor frequency is too low - {minor_freq}, allowd: {min_minor_freq_expected}.', logging.ERROR)\n",
    "    if minor_freq > max_minor_freq_expected:\n",
    "        _log(f'ERROR: minor frequency is too high - {minor_freq}, allowd: {max_minor_freq_expected}.', logging.ERROR)\n",
    "    _log(f'Passed guardrails')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T10:01:25.140339Z",
     "start_time": "2020-12-16T10:01:25.118374Z"
    }
   },
   "outputs": [],
   "source": [
    "# def _calc_pairwise_distances_with_guardrails\n",
    "locus_pairwise_dist, locus_pairwise_counts =  _calc_pairwise_distances_with_guardrails_locus(allels, min_valid_sites_precentage, min_minor_freq_expected, max_minor_freq_expected)\n",
    "# add to slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T13:41:07.422223Z",
     "start_time": "2020-12-16T13:41:07.408224Z"
    }
   },
   "outputs": [],
   "source": [
    "def _write_output_loci_file(output_loci_file, slice_df):\n",
    "    loci_ids = slice_df.columns.values\n",
    "    with open(output_loci_file, 'w') as f:\n",
    "        for locus_id in loci_ids:\n",
    "            f.write(locus_id.replace('_','\\t') + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Func begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T13:41:35.570138Z",
     "start_time": "2020-12-16T13:41:35.548613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr_name is chr9\n",
      "output_dir is C:\\Data\\HUJI\\hgdp\\mock/chr9/\n"
     ]
    }
   ],
   "source": [
    "# outputs:\n",
    "# for each group i in 1,...,k:\n",
    "# 1. {i}_dist_{l}_loci : tab seperated pairwise distances - this is the sum over the valid loci used, and the format in each entry is <sum>,<l'>, or <sum> if l'=l (from the file's name)\n",
    "# the idea here is that in most cases all entries are valid for both individuals, and there is no need to store this value. In the cases there are l'>l valid sites, we add l' to the output in the entry.\n",
    "# dummy example for 3 individuals, over l=5, where individual number 3 has 2 invalid entries (-1 -1 1 0 2)\n",
    "# 0.9 0.7,3\n",
    "# 0.8,3 \n",
    "# the ',3' indicates that there are only 3 valied sited for individuals 1 and 3 and 2 and 3\n",
    "# 2. {i}_loci_ids : <locus_id> <number of non -1 entris>\n",
    "\n",
    "# Validate inputs\n",
    "path_012 = f'{path_to_012_files}{SUFFIX_012}'\n",
    "path_pos = f'{path_to_012_files}{SUFFIX_POS}'\n",
    "_validate_file_exist(path_012)\n",
    "_validate_file_exist(path_pos)\n",
    "\n",
    "# prepare output folder\n",
    "chr_name = _get_chr_name(path_pos)\n",
    "_log(f'chr_name is {chr_name}')\n",
    "output_dir = _prepare_output_folder(base_output_path, chr_name)\n",
    "\n",
    "# load all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T13:41:40.363744Z",
     "start_time": "2020-12-16T13:41:40.319745Z"
    }
   },
   "outputs": [],
   "source": [
    "# get loci positions to use as column names\n",
    "with open(path_pos, 'r') as f:\n",
    "    loci_pos = [l.replace('\\t','_').replace('\\n','') for l in f.readlines()]\n",
    "all_df = pd.read_csv(path_012, sep='\\t', names = ['ind_id'] + loci_pos)\n",
    "# drop first column - individual ids\n",
    "all_df = all_df.drop(columns=['ind_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T13:41:46.631933Z",
     "start_time": "2020-12-16T13:41:46.525666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num of loci: 178\n",
      "Desired size of slice: 10\n",
      "Number of slices: 18\n",
      "Size of last slice: 8\n",
      "\n",
      "#############\n",
      "\n",
      "Slice index: 0. Slice size: 10 \n",
      "Output files: C:\\Data\\HUJI\\hgdp\\mock/chr9/dist_slice_0_10_loci.tsv C:\\Data\\HUJI\\hgdp\\mock/chr9/loci_ids_slice_0.tsv\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '_calc_pairwise_distances_with_guardrails' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1cec9a5beb80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0m_write_output_loci_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_loci_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0m_log\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Output files: {output_dist_file} {output_loci_file}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mpairwise_distances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_calc_pairwise_distances_with_guardrails\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_valid_sites_precentage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_minor_freq_expected\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_minor_freq_expected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0m_write_pairwise_distances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dist_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '_calc_pairwise_distances_with_guardrails' is not defined"
     ]
    }
   ],
   "source": [
    "num_loci = len(all_df.columns)\n",
    "num_of_slices = math.ceil(float(num_loci)/desired_num_of_loci_per_output)\n",
    "size_of_last_slice = num_loci % desired_num_of_loci_per_output\n",
    "_log(f'Total num of loci: {num_loci}')\n",
    "_log(f'Desired size of slice: {desired_num_of_loci_per_output}')\n",
    "_log(f'Number of slices: {num_of_slices}')\n",
    "_log(f'Size of last slice: {size_of_last_slice}')\n",
    "for slice_index in  range(num_of_slices):\n",
    "    min_col_index = slice_index*desired_num_of_loci_per_output\n",
    "    # in case we have less than desired_num_of_loci_per_output, pandas will only take those we have!\n",
    "    max_col_index = min_col_index + desired_num_of_loci_per_output\n",
    "    slice_df = all_df.iloc[:,min_col_index:max_col_index]\n",
    "    num_of_loci = len(slice_df.columns)\n",
    "    _log(f'\\n#############\\n')\n",
    "    _log(f'Slice index: {slice_index}. Slice size: {num_of_loci} ')\n",
    "    output_dist_file = output_dir + OUTPUT_PATTERN_DIST_FILE.format(slice_index=slice_index, num_of_loci=num_of_loci)\n",
    "    output_loci_file = output_dir + OUTPUT_PATTERN_LOCI_FILE.format(slice_index=slice_index)\n",
    "    # todo - maybe write to this file also the number of -1, 0, 1 and 2?\n",
    "    _write_output_loci_file(output_loci_file, slice_df)\n",
    "    _log(f'Output files: {output_dist_file} {output_loci_file}')\n",
    "    pairwise_distances = _calc_pairwise_distances_with_guardrails(slice_df, min_valid_sites_precentage, min_minor_freq_expected, max_minor_freq_expected)\n",
    "    _write_pairwise_distances(output_dist_file, pairwise_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
